{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend ok\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import keras\n",
    "import keras.backend\n",
    "if keras.backend.backend() != 'tensorflow':\n",
    "    raise BaseException(\"This script uses other backend\")\n",
    "else:\n",
    "    keras.backend.set_image_dim_ordering('th')\n",
    "    print(\"Backend ok\")\n",
    "\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.word_vector.thai2vec import get_model\n",
    "from numpy import zeros, array\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from pandas import read_csv\n",
    "from re import compile\n",
    "import pickle\n",
    "\n",
    "word2vec = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(sentence):\n",
    "    return sentence.strip()\n",
    "\n",
    "def remove_tags(sentence):\n",
    "    regex = compile(r'<\\/?[\\w-]+>')\n",
    "    return regex.sub('', sentence)\n",
    "\n",
    "def remove_dash(sentence):\n",
    "    regex = compile(r'-')\n",
    "    return regex.sub('', sentence)\n",
    "\n",
    "def get_word2idx(tokenized_sentences):\n",
    "    word2idx ={}\n",
    "    for sentence in tokenized_sentences:\n",
    "        for word in sentence:\n",
    "            if word not in word2idx:\n",
    "                word2idx[word] = len(word2idx) + 1\n",
    "    word2idx['UNK'] = len(word2idx)\n",
    "    return word2idx\n",
    "\n",
    "def get_index(word, word2idx):\n",
    "    return word2idx[word] if word in word2idx else word2idx['UNK']\n",
    "\n",
    "def get_embeddings(word2vec, word2idx, dim=300):\n",
    "    embeddings = zeros((len(word2idx), dim))\n",
    "    for (word, i) in word2idx.items():\n",
    "        if word != 'UNK' and word in word2vec.index2word:\n",
    "            embeddings[i] = word2vec.word_vec(word)\n",
    "        else:\n",
    "            pass\n",
    "    return embeddings\n",
    "\n",
    "def get_training_data(tokenized_sentences, classes, classes2, word2idx, maxlen=1000, class_num=4, class_num_two=4):\n",
    "    train_x = array([[get_index(word, word2idx) for word in sentence] for sentence in tokenized_sentences])\n",
    "    train_y = [cls for cls in classes]\n",
    "    train_z = [cls for cls in classes2]\n",
    "\n",
    "    train_x = pad_sequences(train_x, maxlen=maxlen, dtype='int32', padding='post', truncating='pre', value=0.)\n",
    "    train_y = array([[1 if i == cls else 0 for i in range(0, class_num)] for cls in train_y])\n",
    "    train_z = array([[1 if i == cls else 0 for i in range(0, class_num_two)] for cls in train_z])\n",
    "\n",
    "    return (train_x, train_y, train_z)\n",
    "\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_object(filename):\n",
    "    r = {}\n",
    "    with open(filename, 'rb') as f:\n",
    "        r = pickle.load(f)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ชิว ฟังสบาย จังหวะเบา', 'ทำงานอยู่ สบายฟังชิวหน่อย', 'จัดมาสัก สบาย ฟังแล้วเพลิน', 'แบบ ฟังแล้วผ่อนคลาย สบายชิวไปวันนี้', 'เหนื่อยแล้วอยากพักผ่อนชิว มาฟังหย่อนใจ', 'ชิว กำลังสบายใจ ชิวชิว', 'แบบเพราะเพลินชิว', ' เบา ฟังสบายจิบกาแฟ', 'เพราะชิว ฟังสบาย', 'ชิวแบบนี้ พอฟังเพลิน เพราะ', 'อกหัก เศร้าบาดใจ ฟังช้า', 'แบบคนโดนเทหน่อย', 'เพิ่งโดนเพื่อนหักหลังมา ปลอบใจหน่อย', 'ตอนนี้เหงา อยากฟังอกหักหน่อย', 'ง่วง เหนื่อย อยากอยู่คนเดียว', 'โดนเท แก้เซ็ง', 'เศร้ามาก  เศร้า', 'แบบ เศร้ากว่านี้ไม่มีอีกแล้ว', 'เศร้า เสียใจมาก', 'เศร้าจับใจ โคตรเหงา', 'เบื่อจัง แก้เซ็งหน่อย', 'จัดเพลิน ฟังแล้วเฮฮาปาร์ตี้', ' rock หน่อย', 'แบบ สนุก ลุยบ้า', 'คึกคัก ฟังแล้วต้องเต้น', 'จัดมานึง มัน ร็อค', 'สนุก เฮฮา', 'แบบมัน แบบสนุกสนาน', 'สนุกสนาน ฟังสนุก', 'มัน ฟังแล้วต้องลุกมาเต้น', 'ที่ทำให้ใจสั่น', 'รักหน่อย', 'เพิ่งคุยกับแฟนมา อยากฟังชิวหน่อย', 'แอบชอบเธอจังเลย หน่อยน้า', 'รักหวานซึ้ง', 'รักแบบเขิน', 'วัยรุ่นใส วัยมีความรัก', 'เขิน ฟังแล้วฟินนนน รักเลย', 'รักใคร่', 'รัก', 'รักแบบหวาน ฟังเพราะ', 'แบบ ลูกทุ่ง หน่อย', 'อยากฟังไทยแท้หน่อย', ' แรปหน่อย', 'หน่อย', 'มันเงียบไป หน่อย', 'อ่านหนังสือหน่อย', 'อะไรก็ได้', 'จัดมาเลย แบบเพราะ']\n",
      "['อยากฟังไทยแท้หน่อย', 'เศร้ามาก  เศร้า', 'โดนเท แก้เซ็ง', 'แบบมัน แบบสนุกสนาน', 'คึกคัก ฟังแล้วต้องเต้น', 'หน่อย', 'รักแบบหวาน ฟังเพราะ']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "mood = []\n",
    "genre = []\n",
    "\n",
    "\n",
    "def get_corpus(path):\n",
    "    global mood, genre\n",
    "    corpus = read_csv(path)\n",
    "    corpus = corpus[['sentence', 'mood', 'genre']]\n",
    "    corpus['sentence'] = corpus['sentence'].apply(clean).apply(remove_tags)\n",
    "    corpus['tokenized_sentence'] = corpus['sentence'].apply(word_tokenize, engine='deepcut')\n",
    "    corpus['genre'] = corpus['genre'].apply(remove_dash)\n",
    "    mood += list(corpus.mood.unique())\n",
    "    genre += list(corpus.genre.unique())\n",
    "    return corpus\n",
    "\n",
    "corpus = get_corpus('kuy.csv')\n",
    "train_corpus = corpus.copy()\n",
    "test_corpus = corpus.sample(n=7,replace=True).copy()\n",
    "\n",
    "\n",
    "mood = sorted(list(set(mood)), reverse=True)\n",
    "genre = sorted(list(set(genre)), reverse=True)\n",
    "mood2idx = dict(zip(mood, [i for i in range(len(mood))]))\n",
    "genre2idx = dict(zip(genre, [i for i in range(len(genre))]))\n",
    "\n",
    "def pos_process(corpus):\n",
    "    global mood2idx, genre2idx\n",
    "    corpus['genre'] = corpus['genre'].apply(lambda x: genre2idx[x])\n",
    "    corpus['mood'] = corpus['mood'].apply(lambda x: mood2idx[x])\n",
    "    return corpus\n",
    "\n",
    "train_corpus = pos_process(train_corpus)\n",
    "test_corpus = pos_process(test_corpus)\n",
    "x_test = np.array(train_corpus['mood'])\n",
    "y_test = np.array(test_corpus['mood'])\n",
    "\n",
    "x = []\n",
    "for i in range(len(train_corpus['sentence'])):\n",
    "    if 'เพลง' in train_corpus['sentence'][i]:\n",
    "        x.append(train_corpus['sentence'][i].replace('เพลง', ''))\n",
    "    else:\n",
    "        x.append(train_corpus['sentence'][i])\n",
    "print(x)\n",
    "\n",
    "x_corpus = []\n",
    "for s in test_corpus['sentence']:\n",
    "    if 'เพลง' in s:\n",
    "        x_corpus.append(s.replace('เพลง', ''))\n",
    "    else:\n",
    "        x_corpus.append(s)\n",
    "print(x_corpus)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 638,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn import linear_model\n",
    "\n",
    "clf = linear_model.LogisticRegression(C=1e5)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "x_train = vectorizer.fit_transform(x).toarray()\n",
    "svd = TruncatedSVD(n_components=50, n_iter=10, random_state=42)\n",
    "x_train = svd.fit_transform(x_train)\n",
    "clf.fit(x_train, x_test) \n",
    "# vectorizer = CountVectorizer()\n",
    "# x_train = vectorizer.fit_transform(train_corpus['sentence'])\n",
    "# x_train = vectorizer.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = vectorizer.transform(x_corpus).toarray()\n",
    "x_test = svd.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 49)"
      ]
     },
     "execution_count": 640,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0, 0, 1, 1, 4, 2])"
      ]
     },
     "execution_count": 641,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0, 0, 1, 1, 4, 2])"
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fm_index/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fm_index/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/fm_index/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fm_index/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.40000000000000002, 1.0, 0.40000000000000002, 0.40000000000000002)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "def evaluate(test_y, pred_y, labels=[i for i in range(0,10)]):\n",
    "    f1 = f1_score(test_y, pred_y, average='macro', labels=labels)\n",
    "    accuracy = accuracy_score(test_y, pred_y)\n",
    "    precision = precision_score(test_y, pred_y, average='macro', labels=labels)\n",
    "    recall = recall_score(test_y, pred_y, average='macro', labels=labels)\n",
    "    return f1, accuracy, precision, recall\n",
    "\n",
    "display(evaluate(y_test, clf.predict(x_test)))\n",
    "# display(evaluate(class_z, pred_z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n"
     ]
    }
   ],
   "source": [
    "t = 'อยากเต้นเฮฮา'\n",
    "if 'ชิว' in t:\n",
    "    ans = np.array([3])\n",
    "elif 'อกหัก' in t:\n",
    "    ans = np.array([0])\n",
    "elif 'สนุก' in t:\n",
    "    ans = np.array([1])\n",
    "else:\n",
    "    t = vectorizer.transform([t]).toarray()\n",
    "    t = svd.transform(t)\n",
    "    ans = clf.predict(t)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all': 4, 'ชิว': 3, 'รัก': 2, 'สนุก': 1, 'เศร้า': 0}"
      ]
     },
     "execution_count": 690,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mood2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
