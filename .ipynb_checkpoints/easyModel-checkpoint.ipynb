{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def clean(sentence):\n",
    "    return sentence.strip()\n",
    "\n",
    "def remove_tags(sentence):\n",
    "    regex = compile(r'<\\/?[\\w-]+>')\n",
    "    return regex.sub('', sentence)\n",
    "\n",
    "def remove_dash(sentence):\n",
    "    regex = compile(r'-')\n",
    "    return regex.sub('', sentence)\n",
    "\n",
    "def get_word2idx(tokenized_sentences):\n",
    "    word2idx ={}\n",
    "    for sentence in tokenized_sentences:\n",
    "        for word in sentence:\n",
    "            if word not in word2idx:\n",
    "                word2idx[word] = len(word2idx) + 1\n",
    "    word2idx['UNK'] = len(word2idx)\n",
    "    return word2idx\n",
    "\n",
    "def get_index(word, word2idx):\n",
    "    return word2idx[word] if word in word2idx else word2idx['UNK']\n",
    "\n",
    "def get_embeddings(word2vec, word2idx, dim=300):\n",
    "    embeddings = zeros((len(word2idx), dim))\n",
    "    for (word, i) in word2idx.items():\n",
    "        if word != 'UNK' and word in word2vec.index2word:\n",
    "            embeddings[i] = word2vec.word_vec(word)\n",
    "        else:\n",
    "            pass\n",
    "    return embeddings\n",
    "\n",
    "def get_training_data(tokenized_sentences, classes, classes2, word2idx, maxlen=1000, class_num=4, class_num_two=4):\n",
    "    train_x = array([[get_index(word, word2idx) for word in sentence] for sentence in tokenized_sentences])\n",
    "    train_y = [cls for cls in classes]\n",
    "    train_z = [cls for cls in classes2]\n",
    "\n",
    "    train_x = pad_sequences(train_x, maxlen=maxlen, dtype='int32', padding='post', truncating='pre', value=0.)\n",
    "    train_y = array([[1 if i == cls else 0 for i in range(0, class_num)] for cls in train_y])\n",
    "    train_z = array([[1 if i == cls else 0 for i in range(0, class_num_two)] for cls in train_z])\n",
    "\n",
    "    return (train_x, train_y, train_z)\n",
    "\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_object(filename):\n",
    "    r = {}\n",
    "    with open(filename, 'rb') as f:\n",
    "        r = pickle.load(f)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-03b8a5b396d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NLP-Corpus_yak.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mtrain_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mtest_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-03b8a5b396d2>\u001b[0m in \u001b[0;36mget_corpus\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mmood\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mood'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'genre'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'read_csv' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "mood = []\n",
    "genre = []\n",
    "\n",
    "\n",
    "def get_corpus(path):\n",
    "    global mood, genre\n",
    "    corpus = read_csv(path)\n",
    "    corpus = corpus[['sentence', 'mood', 'genre']]\n",
    "    corpus['sentence'] = corpus['sentence'].apply(clean).apply(remove_tags)\n",
    "    corpus['tokenized_sentence'] = corpus['sentence'].apply(word_tokenize, engine='deepcut')\n",
    "    corpus['genre'] = corpus['genre'].apply(remove_dash)\n",
    "    mood += list(corpus.mood.unique())\n",
    "    genre += list(corpus.genre.unique())\n",
    "    return corpus\n",
    "\n",
    "corpus = get_corpus('NLP-Corpus_yak.csv')\n",
    "train_corpus = corpus.copy()\n",
    "test_corpus = corpus.sample(n=7,replace=True).copy()\n",
    "\n",
    "\n",
    "mood = sorted(list(set(mood)), reverse=True)\n",
    "genre = sorted(list(set(genre)), reverse=True)\n",
    "mood2idx = dict(zip(mood, [i for i in range(len(mood))]))\n",
    "genre2idx = dict(zip(genre, [i for i in range(len(genre))]))\n",
    "\n",
    "def pos_process(corpus):\n",
    "    global mood2idx, genre2idx\n",
    "    corpus['genre'] = corpus['genre'].apply(lambda x: genre2idx[x])\n",
    "    corpus['mood'] = corpus['mood'].apply(lambda x: mood2idx[x])\n",
    "    return corpus\n",
    "\n",
    "train_corpus = pos_process(train_corpus)\n",
    "test_corpus = pos_process(test_corpus)\n",
    "corpus = pos_process(corpus)\n",
    "\n",
    "print(mood, genre, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
