{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend ok\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import keras\n",
    "import keras.backend\n",
    "if keras.backend.backend() != 'tensorflow':\n",
    "    raise BaseException(\"This script uses other backend\")\n",
    "else:\n",
    "    keras.backend.set_image_dim_ordering('th')\n",
    "    print(\"Backend ok\")\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dropout, Dense, concatenate\n",
    "from keras.regularizers import l2\n",
    "from keras.constraints import max_norm\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.word_vector.thai2vec import get_model\n",
    "from numpy import zeros, array\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from re import compile\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "def clean(sentence):\n",
    "    return sentence.strip()\n",
    "\n",
    "def remove_tags(sentence):\n",
    "    regex = compile(r'<\\/?[\\w-]+>')\n",
    "    return regex.sub('', sentence)\n",
    "\n",
    "def get_word2idx(tokenized_sentences):\n",
    "    word2idx ={}\n",
    "    for sentence in tokenized_sentences:\n",
    "        for word in sentence:\n",
    "            if word not in word2idx:\n",
    "                word2idx[word] = len(word2idx) + 1\n",
    "    word2idx['UNK'] = len(word2idx)\n",
    "    return word2idx\n",
    "\n",
    "def get_index(word, word2idx):\n",
    "    return word2idx[word] if word in word2idx else word2idx['UNK']\n",
    "\n",
    "def get_embeddings(word2vec, word2idx, dim=300):\n",
    "    embeddings = zeros((len(word2idx), dim))\n",
    "    for (word, i) in word2idx.items():\n",
    "        if word != 'UNK' and word in word2vec.index2word:\n",
    "            embeddings[i] = word2vec.word_vec(word)\n",
    "        else:\n",
    "            pass\n",
    "    return embeddings\n",
    "\n",
    "def get_training_data(tokenized_sentences, classes, classes2, word2idx, maxlen=1000, class_num=5, class_num_two=4):\n",
    "    train_x = array([[get_index(word, word2idx) for word in sentence] for sentence in tokenized_sentences])\n",
    "    train_y = [cls for cls in classes]\n",
    "    train_z = [cls for cls in classes2]\n",
    "\n",
    "    train_x = pad_sequences(train_x, maxlen=maxlen, dtype='int32', padding='post', truncating='pre', value=0.)\n",
    "    train_y = array([[1 if i == cls else 0 for i in range(0, class_num)] for cls in train_y])\n",
    "    train_z = array([[1 if i == cls else 0 for i in range(0, class_num_two)] for cls in train_z])\n",
    "\n",
    "    return (train_x, train_y, train_z)\n",
    "\n",
    "def get_data(tokenized_sentences, word2idx, maxlen=1000):\n",
    "    x = array([[get_index(word, word2idx) for word in sentence] for sentence in tokenized_sentences])\n",
    "    x = pad_sequences(x, maxlen=maxlen, dtype='int32', padding='post', truncating='pre', value=0.)\n",
    "    return x\n",
    "\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_object(filename):\n",
    "    r = {}\n",
    "    with open(filename, 'rb') as f:\n",
    "        r = pickle.load(f)\n",
    "    return r    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from numpy import apply_along_axis\n",
    "\n",
    "def prob2class(probs, threshold = 0.3):\n",
    "    resultIdx = []\n",
    "    for x in probs:\n",
    "        maxIdx = max(enumerate(x), key=itemgetter(1))[0]\n",
    "        if x[maxIdx] < threshold:\n",
    "            resultIdx.append(100)\n",
    "        else:\n",
    "            resultIdx.append(maxIdx)\n",
    "    return resultIdx\n",
    "\n",
    "def class2mood(cls):\n",
    "    names = ['เศร้า', 'สนุก', 'รัก', 'ชิว', 'all']\n",
    "    return names[cls] if cls < len(names) else 'All'\n",
    "\n",
    "def class2genre(cls):\n",
    "    names = ['ลูกทุ่ง', 'rock', 'pop', 'hiphop', 'all']\n",
    "    return names[cls] if cls < len(names) else 'All'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights_path = 'models/music_weights.h5'\n",
    "\n",
    "class MusicModel:\n",
    "    def __init__(self):\n",
    "        self.word2vec = get_model()\n",
    "        self.word2idx = load_object('models/word2idx.pkl')\n",
    "        self.embeddings = get_embeddings(self.word2vec, self.word2idx)\n",
    "        self.nn_model = self.get_model(1000, len(self.word2idx), self.embeddings)\n",
    "        self.nn_model.load_weights(weights_path)\n",
    "    \n",
    "    def get_model(self, input_size, vocab_size, embeddings, embedding_dimension=300, class_num=4, class_num_two=4):\n",
    "        inp = Input(shape=(input_size,))\n",
    "        x = Embedding(vocab_size, embedding_dimension, weights=[self.embeddings], trainable=True)(inp)\n",
    "\n",
    "        convs = []\n",
    "        kernel_sizes = [3,4,5]\n",
    "        for kernel_size in kernel_sizes:\n",
    "            conv = Conv1D(100, kernel_size, activation='relu')(x)\n",
    "            pool = GlobalMaxPooling1D()(conv)\n",
    "            convs.append(pool)\n",
    "\n",
    "        x = concatenate(convs)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(100, activation='relu', kernel_regularizer=l2(0.01), kernel_constraint=max_norm(3))(x)\n",
    "        out = Dense(class_num, activation='softmax', name='mood')(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(100, activation='relu', kernel_regularizer=l2(0.01), kernel_constraint=max_norm(3))(x)\n",
    "        aux_out = Dense(class_num_two, activation='softmax', name='genre')(x)\n",
    "\n",
    "        model = Model(inputs=inp, outputs=[out, aux_out])\n",
    "        model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['categorical_accuracy'], loss_weights=[1., 0.2])\n",
    "        return model\n",
    "    \n",
    "    def text2input(self, text):\n",
    "        df = pd.DataFrame(data={'sentence': [text]})\n",
    "        df['tokenized_sentence'] = df['sentence'].apply(word_tokenize, engine='deepcut')\n",
    "        return get_data(df['tokenized_sentence'].values ,self.word2idx)\n",
    "    \n",
    "    def predict(self, text):\n",
    "        pred_y, pred_z = self.nn_model.predict(self.text2input(text))\n",
    "        pred_y = prob2class(pred_y, 0.3)\n",
    "        pred_z = prob2class(pred_z, 0.325)\n",
    "        return class2mood(pred_y[0]), class2genre(pred_z[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Model Instance\n",
    "NN_model = MusicModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('เศร้า', 'pop')\n",
      "('ชิว', 'pop')\n",
      "('ชิว', 'pop')\n"
     ]
    }
   ],
   "source": [
    "#Demo how to use\n",
    "print(NN_model.predict(\"เหงาจัง อยากฟังเพลงเศร้า อยากอยู่คนเดียว\"))\n",
    "print(NN_model.predict(\"ขอเพลงมันๆหน่อย\"))\n",
    "print(NN_model.predict(\"ขอเพลงเต้นๆหน่อย\"))\n",
    "print(NN_model.predict(\"เหงาจัง อยากฟังเพลงเศร้า อยากอยู่คนเดียว\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
